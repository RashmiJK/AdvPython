{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differences between the 'OpenAI SDK' and the 'Azure AI Inference SDK', and how you can use them in LangChain libraries to implement agents.\n",
    "\n",
    "### OpenAI SDK\n",
    "\n",
    "The OpenAI SDK is a Python library that allows you to interact with the OpenAI models like GPT-4, GPT-3 and others.\n",
    "\n",
    "### Azure AI Inference SDK\n",
    "\n",
    "The Azure AI Inference SDK is a Python library that allows you to interact with variety of AI models deployed on Azure including GPT-3, GPT-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI SDK example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install -y openai\n",
    "%conda install -y python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://models.inference.ai.azure.com\",\n",
    "    api_key=os.environ[\"GITHUB_TOKEN\"],\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of France?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=1,\n",
    "    max_tokens=4096,\n",
    "    top_p=1\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure AI Inference SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage\n",
    "from azure.ai.inference.models import UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=\"https://models.inference.ai.azure.com\",\n",
    "    credential=AzureKeyCredential(os.environ[\"GITHUB_TOKEN\"]),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"\"\"\"\"\"),\n",
    "        UserMessage(content=\"Can you explain the basics of machine learning?\"),\n",
    "    ],\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=1,\n",
    "    max_tokens=4096,\n",
    "    top_p=1\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain with Azure AI Inference SDK\n",
    "https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/langchain\n",
    "Terminologies - \n",
    "1. \"Azure AI Model Inference\" is an aPI that exposes a common set of capabilities for foundational models.\n",
    "2. When working with LangChain, install the extension langchain-azure-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install langchain\n",
    "#%pip install langchain_openai\n",
    "%pip install langchain-openai\n",
    "%pip install langchain_azure_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_azure_ai.chat_models import AzureAIChatCompletionsModel\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\"\"\" os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.environ[\"GITHUB_INFERENCE_ENDPOINT\"] \n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.environ[\"GITHUB_TOKEN\"]\n",
    "\n",
    "# Initialize Azure AI Inference \n",
    "azure_ai = AzureChatOpenAI( \n",
    "    azure_deployment=\"gpt-4o\",\n",
    "    model_name=\"gpt-4o\", \n",
    "    api_version=\"gpt-4o-2024-11-20\", \n",
    "    ) \n",
    "\n",
    "# Initialize LangChain with Azure AI Inference \n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that transaltes English to French.Translate the user sentence.\"   \n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"I love programming\"\n",
    "    )\n",
    "]\n",
    "ai_msg = azure_ai.invoke(messages)\n",
    "ai_msg \"\"\"\n",
    "\n",
    "model = AzureAIChatCompletionsModel(\n",
    "    endpoint=os.environ[\"GITHUB_INFERENCE_ENDPOINT\"],\n",
    "    credential=os.environ[\"GITHUB_TOKEN\"],\n",
    "    model_name=\"gpt-4o\",\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(content=\"hi!\"),\n",
    "]\n",
    "model.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://github.com/langchain-ai/langchain-azure/tree/main\n",
    "Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_azure_ai.embeddings import AzureAIEmbeddingsModel\n",
    "\n",
    "emodel = AzureAIEmbeddingsModel (\n",
    "    endpoint=os.environ[\"GITHUB_INFERENCE_ENDPOINT\"],\n",
    "    credential=os.environ[\"GITHUB_TOKEN\"],\n",
    "    model_name=\"cohere-embed-v3-english\",\n",
    ")\n",
    "\n",
    "test_vector = emodel.embed_documents([\"This is a test sentence.\"])\n",
    "print(test_vector)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
